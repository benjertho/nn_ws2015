%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{mathpazo}
\usepackage{placeins}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{listings}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\raggedright \normalfont} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{9pt} % Customize the height of the header
\setlength{\textheight}{640pt} % Customize the height of the header

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Bonn-Rhein-Sieg University of Applied Sciences \\Department of Computer Science\\Neural Networks} \\ [10pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\LARGE  Exercise 5\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}
\date{}
\author{Poulastya Mukherjee, Benjamin Thompson} % Your name

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section{Read Chapter 3.1-3.5 from Haykin's book; summarize or sketch your insights in mind-map or
an outline or a summary.}

\begin{itemize}
\item Rosenblatt proved that given linearly separable data, a perceptron is proven to converge.
\item Least mean square algorithm is the backbone of linear adaptive filters
\item Adaptive filtering
\subitem m dimensional input produces scalar output
\subitem data equally distributed
\subitem data can be spread over space (snapshot) or over time (uniformly spaced in time)
\subitem Filtering process produces the output and error signals
\subitem Adaptive process involves adjustments based on errors
\subitem Error correction is an optimization problem
\item Unconstrained optimization techniques
\subitem Optimal solution is gradient of cost function equal to 0
\item Steepest descent
\subitem converges slowly
\subitem size of eta produces overdamped response when small, under when large
\item Newton method
\subitem needs to be twice continuously differentiable wrt w to form hessian
\subitem converges quickly and generally not subject to underdamped behavior of steepest descent
\subitem Needs to be positive definite matrix, however there is no gaurantee of that.
\item Gauss Newton method
\subitem Only requires jacobian of the error vector as opposed to hessian of cost function
\subitem Jacobian product must be non singular
\item Least mean squares
\subitem Inverse of the learning rate eta is the 
\subitem weight vector traverses random trajectory in contrast with steepest descent
\subitem The stability of the system is determined by choosing an appropriate eta for x
\subitem Model independent, therefore robust
\subitem Needs approx 10x the dimensionality iterations to converge
\end{itemize}

\section{ (3.1) }
    
\subsection{ (a) }
%\FloatBarrier
%\begin{figure}[h]
%\centering
%\includegraphics[width=0.4\textwidth]{ex1_im1}
%\caption{Piece-wise linear activation function [Haykin]}
%\end{figure}
%\FloatBarrier

\subsection{ (b) }

\section{ (3.2) }
\section{ (3.4) }
\section{ (3.8) }
\subsection{ (a) }
\subsection{ (b) }
\subsection{ (c) }


\end{document}